"""
Research Orchestrator for the Nexus Agents system.

This orchestrator manages the end-to-end research workflow:
Query → Topic Decomposition → Planning → Search → Synthesis → Report
"""
import asyncio
import json
import os
import uuid
from datetime import datetime, timezone
from typing import Any, Dict, List, Optional
from enum import Enum

from src.agents.research.topic_decomposer_agent import TopicDecomposerAgent
from src.agents.research.planning_agent import ResearchPlanningAgent
from src.agents.search.firecrawl_agent import FirecrawlSearchAgent
from src.agents.search.exa_agent import ExaSearchAgent
from src.agents.search.perplexity_agent import PerplexitySearchAgent
from src.agents.search.linkup_agent import LinkUpSearchAgent
from src.agents.summarization.reasoning_agent import ReasoningAgent
from src.agents.summarization.summarization_agent import SummarizationAgent
from src.agents.research.dok_workflow_orchestrator import DOKWorkflowOrchestrator
from src.orchestration.communication_bus import CommunicationBus, Message
from src.llm import LLMClient
from src.persistence.postgres_knowledge_base import PostgresKnowledgeBase


class ResearchStatus(Enum):
    """Research task status."""
    PENDING = "pending"
    DECOMPOSING = "decomposing"
    PLANNING = "planning"
    SEARCHING = "searching"
    SUMMARIZING = "summarizing"  # DOK Level 1: Source summarization
    BUILDING_KNOWLEDGE = "building_knowledge"  # DOK Level 2: Knowledge tree
    GENERATING_INSIGHTS = "generating_insights"  # DOK Level 3: Insights
    ANALYZING_POVS = "analyzing_povs"  # DOK Level 4: Spiky POVs
    ANALYZING = "analyzing"
    SYNTHESIZING = "synthesizing"
    COMPLETED = "completed"
    FAILED = "failed"


class ResearchOrchestrator:
    """
    Orchestrates the complete research workflow using existing agents.
    
    Based on the proven workflow pattern from test_live_research_workflow.py.backup:
    1. Topic Decomposition
    2. Research Planning  
    3. Search Execution (MCP servers)
    4. Content Analysis
    5. Final Synthesis & Report Generation
    """
    
    def __init__(self, 
                 communication_bus: CommunicationBus,
                 llm_client: LLMClient,
                 knowledge_base: PostgresKnowledgeBase):
        self.bus = communication_bus
        self.communication_bus = communication_bus  # For test compatibility
        self.llm_client = llm_client
        self.knowledge_base = knowledge_base
        self.active_research_tasks = {}
        
        # Initialize DOK workflow orchestrator with proper repository
        from src.database.dok_taxonomy_repository import DOKTaxonomyRepository
        dok_repository = DOKTaxonomyRepository(knowledge_base)
        self.dok_orchestrator = DOKWorkflowOrchestrator(llm_client, dok_repository)
        
        # Initialize search agents
        self.search_agents = {
            'firecrawl': FirecrawlSearchAgent(
                agent_id="firecrawl_agent",
                name="Firecrawl Search Agent",
                description="Web search and crawling agent using Firecrawl",
                communication_bus=self.communication_bus,
                llm_client=llm_client,
                firecrawl_api_key=os.getenv('FIRECRAWL_API_KEY', 'test_key')
            ),
            'exa': ExaSearchAgent(
                agent_id="exa_agent",
                name="Exa Search Agent",
                description="Web search agent using Exa",
                communication_bus=self.communication_bus,
                llm_client=llm_client,
                exa_api_key=os.getenv('EXA_API_KEY', 'test_key')
            ),
            'perplexity': PerplexitySearchAgent(
                agent_id="perplexity_agent",
                name="Perplexity Search Agent",
                description="AI-powered search agent using Perplexity",
                communication_bus=self.communication_bus,
                llm_client=llm_client,
                perplexity_api_key=os.getenv('PERPLEXITY_API_KEY', 'test_key')
            ),
            'linkup': LinkUpSearchAgent(
                agent_id="linkup_agent",
                name="LinkUp Search Agent",
                description="Web search agent using LinkUp",
                communication_bus=self.communication_bus,
                llm_client=llm_client,
                linkup_api_key=os.getenv('LINKUP_API_KEY', 'test_key')
            )
        }
        
    async def start_research_task(self, 
                                research_query: str,
                                user_id: str = None) -> str:
        """
        Start a new research task.
        
        Args:
            research_query: The research question to investigate
            user_id: Optional user identifier
            
        Returns:
            Task ID for tracking progress
        """
        task_id = str(uuid.uuid4())
        
        # Store initial task in database
        await self.knowledge_base.store_research_task(
            task_id=task_id,
            research_query=research_query,
            status=ResearchStatus.PENDING.value,
            user_id=user_id,
            created_at=datetime.now(timezone.utc)
        )
        
        # Start the workflow
        asyncio.create_task(self._execute_research_workflow(task_id, research_query))
        
        return task_id
    
    async def _execute_research_workflow(self, task_id: str, research_query: str):
        """Execute the complete research workflow."""
        try:
            # Step 1: Topic Decomposition
            await self._update_task_status(task_id, ResearchStatus.DECOMPOSING)
            decomposition = await self._decompose_topic(research_query)
            
            # Step 2: Research Planning  
            await self._update_task_status(task_id, ResearchStatus.PLANNING)
            plan = await self._create_research_plan(decomposition, research_query)
            
            # Step 3: Search Execution
            await self._update_task_status(task_id, ResearchStatus.SEARCHING)
            search_results = await self._execute_searches(plan, research_query)
            
            # Step 4: DOK Taxonomy Workflow
            await self._update_task_status(task_id, ResearchStatus.SUMMARIZING)
            dok_workflow_result = await self._execute_dok_workflow(task_id, search_results, research_query)
            
            # Step 5: Content Analysis (now using DOK taxonomy data)
            await self._update_task_status(task_id, ResearchStatus.ANALYZING)
            analysis = await self._analyze_content_with_dok(dok_workflow_result, research_query)
            
            # Step 6: Final Synthesis & Report
            await self._update_task_status(task_id, ResearchStatus.SYNTHESIZING)
            final_report = await self._synthesize_report_with_dok(
                research_query, decomposition, dok_workflow_result, analysis
            )
            
            # Store final report in database
            await self.knowledge_base.store_research_report(
                task_id=task_id,
                report_markdown=final_report,
                metadata={
                    "decomposition": decomposition,
                    "plan": plan,
                    "search_results_count": len(search_results),
                    "analysis_summary": analysis.get("summary", ""),
                    "dok_workflow_stats": dok_workflow_result.workflow_stats,
                    "total_sources": dok_workflow_result.workflow_stats.get("total_sources", 0),
                    "total_insights": dok_workflow_result.workflow_stats.get("total_insights", 0),
                    "total_spiky_povs": dok_workflow_result.workflow_stats.get("total_spiky_povs", 0)
                }
            )
            
            await self._update_task_status(task_id, ResearchStatus.COMPLETED)
            
        except Exception as e:
            await self._update_task_status(task_id, ResearchStatus.FAILED, str(e))
            raise
    
    async def _decompose_topic(self, research_query: str) -> Dict[str, Any]:
        """Decompose research topic using TopicDecomposerAgent pattern."""
        # Implementation based on test_live_research_workflow.py.backup
        prompt = f"""
        Please decompose the following research query into a hierarchical structure:
        
        Research Query: {research_query}
        
        Return a JSON object with main topic, subtopics, and key questions.
        """
        
        response = await self.llm_client.generate(prompt, use_reasoning_model=True)
        return json.loads(response)
    
    async def _create_research_plan(self, decomposition: Dict[str, Any], research_query: str) -> Dict[str, Any]:
        """Create research plan using ResearchPlanningAgent pattern."""
        # Implementation leverages existing ResearchPlanningAgent logic
        return {
            "tasks": [],
            "search_strategies": ["web_search", "academic_search", "news_search"],
            "priority_topics": decomposition.get("subtopics", [])
        }
    
    async def _execute_searches(self, plan: Dict[str, Any], research_query: str) -> List[Dict[str, Any]]:
        """Execute searches using MCP-integrated search agents."""
        # Implementation uses existing search agents with MCP clients
        results = []
        # This will integrate with the working search agents we have
        return results
    
    async def _execute_dok_workflow(self, task_id: str, search_results: List[Dict[str, Any]], research_query: str) -> Any:
        """Execute the complete DOK taxonomy workflow."""
        # Convert search results to the format expected by DOK orchestrator
        sources = []
        for result in search_results:
            sources.append({
                'content': result.get('content', ''),
                'metadata': {
                    'source_id': result.get('source_id', f"src_{uuid.uuid4().hex[:8]}"),
                    'title': result.get('title', 'Unknown'),
                    'url': result.get('url', ''),
                    'provider': result.get('provider', 'unknown')
                }
            })
        
        # Execute DOK workflow with status updates
        await self._update_task_status(task_id, ResearchStatus.BUILDING_KNOWLEDGE)
        await self._update_task_status(task_id, ResearchStatus.GENERATING_INSIGHTS)
        await self._update_task_status(task_id, ResearchStatus.ANALYZING_POVS)
        
        return await self.dok_orchestrator.execute_complete_workflow(
            task_id=task_id,
            sources=sources,
            research_context=research_query
        )
    
    async def _analyze_content_with_dok(self, dok_workflow_result: Any, research_query: str) -> Dict[str, Any]:
        """Analyze content using DOK taxonomy data."""
        # Extract key analysis from DOK workflow result
        return {
            "summary": f"Analysis of {dok_workflow_result.workflow_stats['total_sources']} sources complete",
            "key_findings": [insight['insight_text'] for insight in dok_workflow_result.insights[:5]],
            "dok_stats": dok_workflow_result.workflow_stats,
            "knowledge_tree_nodes": len(dok_workflow_result.knowledge_tree),
            "insights_generated": len(dok_workflow_result.insights),
            "spiky_povs_generated": len(dok_workflow_result.spiky_povs['truth']) + len(dok_workflow_result.spiky_povs['myth'])
        }
    
    async def _analyze_content(self, search_results: List[Dict[str, Any]], research_query: str) -> Dict[str, Any]:
        """Analyze search content using reasoning agents (legacy method)."""
        # Implementation uses existing ReasoningAgent
        return {"summary": "Analysis complete", "key_findings": []}
    
    async def _synthesize_report_with_dok(self, research_query: str, decomposition: Dict[str, Any], 
                                        dok_workflow_result: Any, analysis: Dict[str, Any]) -> str:
        """Synthesize final markdown report with DOK taxonomy data."""
        # Track sources used in each report section
        all_source_ids = [summary.source_id for summary in dok_workflow_result.source_summaries]
        
        # Track section usage for bibliography
        await self.dok_orchestrator.track_section_sources(
            task_id=dok_workflow_result.task_id,
            section_type='key_findings',
            source_ids=all_source_ids[:len(all_source_ids)//2]  # First half for key findings
        )
        
        await self.dok_orchestrator.track_section_sources(
            task_id=dok_workflow_result.task_id,
            section_type='evidence_analysis',
            source_ids=all_source_ids[len(all_source_ids)//2:]  # Second half for evidence
        )
        
        # Create comprehensive report using DOK taxonomy data
        insights_text = "\n".join([
            f"- **{insight['category']}**: {insight['insight_text']}"
            for insight in dok_workflow_result.insights
        ])
        
        spiky_povs_text = ""
        if dok_workflow_result.spiky_povs['truth']:
            spiky_povs_text += "\n### Truths\n"
            spiky_povs_text += "\n".join([
                f"- {pov['statement']}\n  - *Reasoning*: {pov['reasoning']}"
                for pov in dok_workflow_result.spiky_povs['truth']
            ])
        
        if dok_workflow_result.spiky_povs['myth']:
            spiky_povs_text += "\n### Myths\n"
            spiky_povs_text += "\n".join([
                f"- {pov['statement']}\n  - *Reasoning*: {pov['reasoning']}"
                for pov in dok_workflow_result.spiky_povs['myth']
            ])
        
        knowledge_tree_text = "\n".join([
            f"- **{node['category']}**: {node['summary']}"
            for node in dok_workflow_result.knowledge_tree
        ])
        
        bibliography_text = "\n".join([
            f"- [{source['title']}]({source['url']}) - {source['provider']}"
            for source in dok_workflow_result.bibliography.get('sources', [])
        ])
        
        prompt = f"""
        Create a comprehensive research report in markdown format for:
        
        **Research Query**: {research_query}
        
        Use the following DOK taxonomy analysis to structure your report:
        
        **Knowledge Tree (DOK Levels 1-2)**:
        {knowledge_tree_text}
        
        **Strategic Insights (DOK Level 3)**:
        {insights_text}
        
        **Spiky Points of View (DOK Level 4)**:
        {spiky_povs_text}
        
        **Bibliography**:
        {bibliography_text}
        
        **Analysis Stats**:
        - Total Sources: {dok_workflow_result.workflow_stats['total_sources']}
        - DOK1 Facts: {dok_workflow_result.workflow_stats['total_dok1_facts']}
        - Knowledge Nodes: {dok_workflow_result.workflow_stats['knowledge_tree_nodes']}
        - Insights Generated: {dok_workflow_result.workflow_stats['total_insights']}
        - Spiky POVs: {dok_workflow_result.workflow_stats['total_spiky_povs']}
        
        Structure the report with these sections:
        1. Executive Summary
        2. Key Findings (based on DOK 1-2 knowledge tree)
        3. Evidence Analysis (based on DOK 3 insights)
        4. Causal Relationships (synthesized from insights)
        5. Alternative Interpretations (based on DOK 4 spiky POVs)
        6. Conclusions
        7. Bibliography
        
        Make the report comprehensive, well-structured, and evidence-based.
        """
        
        return await self.llm_client.generate(prompt, use_reasoning_model=True)
    
    async def _synthesize_report(self, research_query: str, decomposition: Dict[str, Any], 
                               search_results: List[Dict[str, Any]], analysis: Dict[str, Any]) -> str:
        """Synthesize final markdown report (legacy method)."""
        # Implementation based on test_final_synthesis from backup file
        prompt = f"""
        Create a comprehensive research report in markdown format for:
        
        Research Query: {research_query}
        
        Include: Executive Summary, Key Findings, Detailed Analysis, Conclusions, Sources
        """
        
        return await self.llm_client.generate(prompt, use_reasoning_model=True)
    
    async def _update_task_status(self, task_id: str, status: ResearchStatus, error_message: str = None):
        """Update task status in database."""
        await self.knowledge_base.update_research_task_status(
            task_id=task_id,
            status=status.value,
            error_message=error_message,
            updated_at=datetime.now(timezone.utc)
        )
    
    async def get_research_task_status(self, task_id: str) -> Dict[str, Any]:
        """Get current status of a research task."""
        return await self.knowledge_base.get_research_task(task_id)
    
    async def get_research_report(self, task_id: str) -> Optional[str]:
        """Get final markdown report for a completed research task."""
        task = await self.knowledge_base.get_research_task(task_id)
        if task and task.get("status") == ResearchStatus.COMPLETED.value:
            return await self.knowledge_base.get_research_report(task_id)
        return None
