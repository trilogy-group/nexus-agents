"""
Unit and integration tests for DOKWorkflowOrchestrator.
Tests the complete DOK taxonomy workflow from sources to Spiky POVs.
"""
import pytest
import asyncio
import uuid
import sys
import os
from pathlib import Path
from unittest.mock import Mock, AsyncMock, patch
from datetime import datetime, timezone

# Add the src directory to the path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from src.agents.research.dok_workflow_orchestrator import DOKWorkflowOrchestrator, DOKWorkflowResult
from src.agents.research.summarization_agent import SourceSummary
from src.database.dok_taxonomy_repository import DOKTaxonomyRepository


@pytest.fixture
def mock_llm_client():
    """Mock LLM client for testing."""
    client = Mock()
    client.generate = AsyncMock()
    return client


@pytest.fixture
def mock_dok_repository():
    """Mock DOK taxonomy repository for testing."""
    repo = Mock(spec=DOKTaxonomyRepository)
    repo.store_source_summary = AsyncMock(return_value=True)
    repo.create_knowledge_node = AsyncMock(return_value="node_123")
    repo.link_sources_to_knowledge_node = AsyncMock(return_value=True)
    repo.create_insight = AsyncMock(return_value="insight_456")
    repo.create_spiky_pov = AsyncMock(return_value="pov_789")
    repo.get_bibliography_by_task = AsyncMock(return_value={
        "sources": [],
        "total_sources": 0,
        "section_usage": {}
    })
    repo.track_section_sources = AsyncMock(return_value=True)
    return repo


@pytest.fixture
def dok_orchestrator(mock_llm_client, mock_dok_repository):
    """Fixture to provide a DOKWorkflowOrchestrator instance."""
    orchestrator = DOKWorkflowOrchestrator(
        llm_client=mock_llm_client,
        dok_repository=mock_dok_repository
    )
    return orchestrator


@pytest.fixture
def sample_sources():
    """Sample sources for testing."""
    return [
        {
            'content': 'The Model Context Protocol (MCP) is a standardized way for AI applications to connect to external data sources and tools.',
            'metadata': {
                'source_id': 'src_001',
                'title': 'MCP Introduction',
                'url': 'https://example.com/mcp-intro',
                'provider': 'anthropic'
            }
        },
        {
            'content': 'AI agent collaboration requires standardized communication protocols to ensure reliable inter-agent coordination.',
            'metadata': {
                'source_id': 'src_002',
                'title': 'Agent Collaboration',
                'url': 'https://example.com/agent-collab',
                'provider': 'research'
            }
        }
    ]


@pytest.mark.unit
class TestDOKWorkflowOrchestrator:
    """Unit tests for DOKWorkflowOrchestrator."""
    
    def test_initialization(self, mock_llm_client, mock_dok_repository):
        """Test DOKWorkflowOrchestrator initialization."""
        orchestrator = DOKWorkflowOrchestrator(
            llm_client=mock_llm_client,
            dok_repository=mock_dok_repository
        )
        
        assert orchestrator.llm_client == mock_llm_client
        assert orchestrator.dok_repository == mock_dok_repository
        assert orchestrator.summarization_agent is not None
    
    def test_initialization_with_defaults(self):
        """Test initialization with default dependencies."""
        with patch('src.agents.research.dok_workflow_orchestrator.DOKTaxonomyRepository') as mock_repo_class:
            
            mock_repo = Mock()
            mock_repo_class.return_value = mock_repo
            
            orchestrator = DOKWorkflowOrchestrator()
            
            assert orchestrator.llm_client is None  # No default LLM client
            assert orchestrator.dok_repository == mock_repo
            mock_repo_class.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_summarize_sources(self, dok_orchestrator, sample_sources):
        """Test source summarization phase."""
        # Mock summarization agent
        mock_summaries = [
            SourceSummary(
                summary_id="sum_001",
                source_id="src_001",
                subtask_id="subtask_test",
                dok1_facts=["MCP is a protocol", "Used for AI integration"],
                summary="MCP overview summary",
                summarized_by="test_agent",
                created_at=datetime.now(timezone.utc)
            ),
            SourceSummary(
                summary_id="sum_002",
                source_id="src_002",
                subtask_id="subtask_test",
                dok1_facts=["Agents need coordination", "Protocols are essential"],
                summary="Agent collaboration summary",
                summarized_by="test_agent",
                created_at=datetime.now(timezone.utc)
            )
        ]
        
        dok_orchestrator.summarization_agent.batch_summarize_sources = AsyncMock(return_value=mock_summaries)
        
        # Test summarization
        result = await dok_orchestrator._summarize_sources(
            sample_sources, "AI interoperability research", "subtask_test"
        )
        
        assert len(result) == 2
        assert all(isinstance(s, SourceSummary) for s in result)
        assert dok_orchestrator.dok_repository.store_source_summary.call_count == 2
    
    @pytest.mark.asyncio
    async def test_categorize_summaries(self, dok_orchestrator):
        """Test summary categorization."""
        # Mock LLM response for categorization
        categorization_response = '{"AI Protocols": [0], "Agent Coordination": [1]}'
        dok_orchestrator.llm_client.generate.return_value = categorization_response
        
        sample_summaries = [
            SourceSummary(
                summary_id="sum_001",
                source_id="src_001",
                subtask_id=None,
                dok1_facts=["MCP fact"],
                summary="MCP summary",
                summarized_by="agent",
                created_at=datetime.now(timezone.utc)
            ),
            SourceSummary(
                summary_id="sum_002",
                source_id="src_002",
                subtask_id=None,
                dok1_facts=["Coordination fact"],
                summary="Coordination summary",
                summarized_by="agent",
                created_at=datetime.now(timezone.utc)
            )
        ]
        
        result = await dok_orchestrator._categorize_summaries(
            sample_summaries, "AI research context"
        )
        
        assert "AI Protocols" in result
        assert "Agent Coordination" in result
        assert len(result["AI Protocols"]) == 1
        assert len(result["Agent Coordination"]) == 1
    
    @pytest.mark.asyncio
    async def test_categorize_summaries_json_error(self, dok_orchestrator):
        """Test summary categorization with JSON parsing error."""
        # Mock invalid JSON response
        dok_orchestrator.llm_client.generate.return_value = "Invalid JSON"
        
        sample_summaries = [SourceSummary(
            summary_id="sum_001",
            source_id="src_001",
            subtask_id=None,
            dok1_facts=["fact"],
            summary="summary",
            summarized_by="agent",
            created_at=datetime.now(timezone.utc)
        )]
        
        result = await dok_orchestrator._categorize_summaries(
            sample_summaries, "test context"
        )
        
        # Should fallback to single category
        assert "Research Sources" in result
        assert len(result["Research Sources"]) == 1
    
    @pytest.mark.asyncio
    async def test_create_category_summary(self, dok_orchestrator):
        """Test category summary creation."""
        mock_summary_response = "Comprehensive analysis of AI protocol standardization efforts."
        dok_orchestrator.llm_client.generate.return_value = mock_summary_response
        
        sample_summaries = [
            SourceSummary(
                summary_id="sum_001",
                source_id="src_001",
                subtask_id=None,
                dok1_facts=["fact1"],
                summary="Summary 1",
                summarized_by="agent",
                created_at=datetime.now(timezone.utc)
            )
        ]
        
        result = await dok_orchestrator._create_category_summary(
            "AI Protocols", sample_summaries, "AI research"
        )
        
        assert result == mock_summary_response
        dok_orchestrator.llm_client.generate.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_create_category_summary_llm_error(self, dok_orchestrator):
        """Test category summary creation with LLM error."""
        dok_orchestrator.llm_client.generate.side_effect = Exception("LLM error")
        
        sample_summaries = [SourceSummary(
            summary_id="sum_001",
            source_id="src_001",
            subtask_id=None,
            dok1_facts=["fact"],
            summary="summary",
            summarized_by="agent",
            created_at=datetime.now(timezone.utc)
        )]
        
        result = await dok_orchestrator._create_category_summary(
            "Test Category", sample_summaries, "test context"
        )
        
        assert "Summary of 1 sources in Test Category" in result
    
    @pytest.mark.asyncio
    async def test_generate_insights(self, dok_orchestrator):
        """Test insight generation."""
        # Mock LLM response for insights
        insights_response = '''[
            {
                "category": "AI Interoperability",
                "insight": "MCP represents a paradigm shift in AI system integration.",
                "evidence_summary": "Multiple sources confirm standardization benefits",
                "confidence": 0.85
            }
        ]'''
        dok_orchestrator.llm_client.generate.return_value = insights_response
        
        sample_summaries = [SourceSummary(
            summary_id="sum_001",
            source_id="src_001",
            subtask_id=None,
            dok1_facts=["MCP standardizes connections"],
            summary="MCP analysis",
            summarized_by="agent",
            created_at=datetime.now(timezone.utc)
        )]
        
        knowledge_tree = [
            {"category": "AI Protocols", "summary": "Protocol analysis"}
        ]
        
        result = await dok_orchestrator._generate_insights(
            "task_123", sample_summaries, knowledge_tree, "AI research"
        )
        
        assert len(result) == 1
        assert result[0]["category"] == "AI Interoperability"
        dok_orchestrator.dok_repository.create_insight.assert_called_once()
    
    @pytest.mark.asyncio
    async def test_generate_insights_json_error(self, dok_orchestrator):
        """Test insight generation with JSON parsing error."""
        dok_orchestrator.llm_client.generate.return_value = "Invalid JSON"
        
        result = await dok_orchestrator._generate_insights(
            "task_123", [], [], "context"
        )
        
        assert result == []
    
    @pytest.mark.asyncio
    async def test_analyze_spiky_povs(self, dok_orchestrator):
        """Test Spiky POV analysis."""
        # Mock LLM response for POVs
        povs_response = '''{
            "truths": [
                {
                    "statement": "AI standardization is inevitable",
                    "reasoning": "Historical patterns show technology convergence"
                }
            ],
            "myths": [
                {
                    "statement": "AI systems will naturally interoperate",
                    "reasoning": "Evidence shows explicit protocols are required"
                }
            ]
        }'''
        dok_orchestrator.llm_client.generate.return_value = povs_response
        
        sample_insights = [
            {"insight_id": "insight_123", "category": "AI Protocols", "insight_text": "Test insight"}
        ]
        
        result = await dok_orchestrator._analyze_spiky_povs(
            "task_123", sample_insights, "AI research"
        )
        
        assert "truth" in result
        assert "myth" in result
        assert len(result["truth"]) == 1
        assert len(result["myth"]) == 1
        
        # Verify POV creation calls
        assert dok_orchestrator.dok_repository.create_spiky_pov.call_count == 2
    
    @pytest.mark.asyncio
    async def test_analyze_spiky_povs_json_error(self, dok_orchestrator):
        """Test Spiky POV analysis with JSON parsing error."""
        dok_orchestrator.llm_client.generate.return_value = "Invalid JSON"
        
        result = await dok_orchestrator._analyze_spiky_povs(
            "task_123", [], "context"
        )
        
        assert result == {"truth": [], "myth": []}
    
    def test_compile_workflow_stats(self, dok_orchestrator):
        """Test workflow statistics compilation."""
        source_summaries = [
            SourceSummary(
                summary_id="sum_001",
                source_id="src_001",
                subtask_id=None,
                dok1_facts=["fact1", "fact2"],
                summary="summary1",
                summarized_by="agent",
                created_at=datetime.now(timezone.utc)
            ),
            SourceSummary(
                summary_id="sum_002",
                source_id="src_002",
                subtask_id=None,
                dok1_facts=["fact3"],
                summary="summary2",
                summarized_by="agent",
                created_at=datetime.now(timezone.utc)
            )
        ]
        
        knowledge_tree = [{"node_id": "node1"}, {"node_id": "node2"}]
        insights = [{"insight_id": "insight1"}]
        spiky_povs = {
            "truth": [{"pov_id": "pov1"}],
            "myth": [{"pov_id": "pov2"}, {"pov_id": "pov3"}]
        }
        
        stats = dok_orchestrator._compile_workflow_stats(
            source_summaries, knowledge_tree, insights, spiky_povs
        )
        
        assert stats["total_sources"] == 2
        assert stats["total_dok1_facts"] == 3
        assert stats["avg_facts_per_source"] == 1.5
        assert stats["knowledge_tree_nodes"] == 2
        assert stats["total_insights"] == 1
        assert stats["spiky_povs_truths"] == 1
        assert stats["spiky_povs_myths"] == 2
        assert stats["total_spiky_povs"] == 3
        assert stats["workflow_completion"] is True
    
    @pytest.mark.asyncio
    async def test_track_section_sources(self, dok_orchestrator):
        """Test tracking section sources."""
        # Mock the repository method to return True
        dok_orchestrator.dok_repository.track_report_section_sources.return_value = True
        
        result = await dok_orchestrator.track_section_sources(
            "task_123", "key_findings", ["src_001", "src_002"]
        )
        
        assert result is True
        dok_orchestrator.dok_repository.track_report_section_sources.assert_called_once_with(
            "task_123", "key_findings", ["src_001", "src_002"]
        )


@pytest.mark.integration
@pytest.mark.asyncio
class TestDOKWorkflowOrchestratorIntegration:
    """Integration tests for DOKWorkflowOrchestrator."""
    
    async def test_execute_complete_workflow_mock(self, dok_orchestrator, sample_sources):
        """Test complete workflow execution with mocked dependencies."""
        task_id = f"test_task_{uuid.uuid4().hex[:8]}"
        research_context = "AI interoperability and protocol standardization"
        
        # Mock all LLM responses
        def mock_llm_response(prompt):
            if "Extract factual statements" in prompt:
                return '["Fact 1", "Fact 2"]'
            elif "Categorize the following" in prompt:
                return '{"AI Protocols": [0], "System Integration": [1]}'
            elif "Create a comprehensive summary" in prompt:
                return "Comprehensive category summary"
            elif "Generate 3-5 strategic insights" in prompt:
                return '''[{
                    "category": "Protocol Analysis",
                    "insight": "Test insight",
                    "evidence_summary": "Test evidence",
                    "confidence": 0.9
                }]'''
            elif 'Generate "Spiky POVs"' in prompt:
                return '''{
                    "truths": [{"statement": "Truth", "reasoning": "Truth reasoning"}],
                    "myths": [{"statement": "Myth", "reasoning": "Myth reasoning"}]
                }'''
            else:
                return "Default response"
        
        dok_orchestrator.llm_client.generate.side_effect = mock_llm_response
        
        # Execute complete workflow
        result = await dok_orchestrator.execute_complete_workflow(
            task_id=task_id,
            sources=sample_sources,
            research_context=research_context
        )
        
        # Verify result structure
        assert isinstance(result, DOKWorkflowResult)
        assert result.task_id == task_id
        assert len(result.source_summaries) == 2
        assert len(result.knowledge_tree) >= 1
        assert len(result.insights) >= 1
        assert "truth" in result.spiky_povs
        assert "myth" in result.spiky_povs
        assert "workflow_completion" in result.workflow_stats
    
    async def test_execute_complete_workflow_error_handling(self, dok_orchestrator, sample_sources):
        """Test workflow error handling."""
        task_id = f"error_test_{uuid.uuid4().hex[:8]}"
        
        # Mock LLM to raise an error
        dok_orchestrator.llm_client.generate.side_effect = Exception("LLM API error")
        
        # Should complete with error handling (no exception raised)
        result = await dok_orchestrator.execute_complete_workflow(
            task_id=task_id,
            sources=sample_sources,
            research_context="test context"
        )
        
        # Verify that workflow completed despite errors
        assert result is not None
        assert result.task_id == task_id
        assert result.workflow_stats["workflow_completion"] is True
        # Verify that summaries contain error messages
        assert len(result.source_summaries) > 0
        assert any("processing error" in summary.summary for summary in result.source_summaries)


@pytest.mark.integration
@pytest.mark.postgres
@pytest.mark.asyncio
async def test_dok_workflow_orchestrator_end_to_end():
    """End-to-end integration test with real database (but mocked LLM)."""
    try:
        from src.persistence.postgres_knowledge_base import PostgresKnowledgeBase
        
        # Initialize real components
        kb = PostgresKnowledgeBase()
        await kb.connect()
        
        # Create test task
        task_id = f"e2e_workflow_{uuid.uuid4().hex[:8]}"
        await kb.create_task(
            task_id=task_id,
            query="E2E workflow test",
            title="E2E Workflow Test",
            description="End-to-end workflow test for DOK taxonomy"
        )
        
        # Create orchestrator with mocked LLM
        mock_llm = AsyncMock()
        
        async def mock_llm_response(prompt):
            if "Extract factual statements from the following source content" in prompt:
                return '["E2E fact 1", "E2E fact 2"]'
            elif "Create a concise summary of the following source content" in prompt:
                return "E2E testing validates complete system behavior and integration patterns."
            elif "Categorize the following source summaries" in prompt:
                return '{"E2E Testing": [0]}'
            elif "Create a comprehensive summary of the following sources within the" in prompt:
                return "E2E category summary for testing methodology"
            elif "Generate 3-5 strategic insights" in prompt:
                return '''[{
                    "category": "E2E Analysis",
                    "insight": "E2E workflow demonstrates system integration",
                    "evidence_summary": "Test evidence",
                    "confidence": 0.95
                }]'''
            elif "Generate \"Spiky POVs\"" in prompt:
                return '''{
                    "truths": [{"statement": "E2E testing is essential", "reasoning": "Validates integration"}],
                    "myths": [{"statement": "Unit tests are sufficient", "reasoning": "Integration gaps exist"}]
                }'''
            else:
                return "Default E2E response"
        
        mock_llm.generate.side_effect = mock_llm_response
        
        orchestrator = DOKWorkflowOrchestrator(llm_client=mock_llm)
        
        # Mock DOK repository methods for knowledge tree creation
        orchestrator.dok_repository.create_knowledge_node = AsyncMock(return_value="node_123")
        orchestrator.dok_repository.link_sources_to_knowledge_node = AsyncMock(return_value=True)
        orchestrator.dok_repository.create_insight = AsyncMock(return_value="insight_456")
        orchestrator.dok_repository.create_spiky_pov = AsyncMock(return_value="pov_789")
        orchestrator.dok_repository.get_bibliography_by_task = AsyncMock(return_value={
            'sources': [],
            'total_sources': 0,
            'section_usage': {}
        })
        
        # Test sources
        sources = [{
            'content': 'End-to-end testing validates complete system behavior and integration.',
            'metadata': {
                'source_id': f'e2e_src_{uuid.uuid4().hex[:8]}',
                'title': 'E2E Testing Guide',
                'url': 'https://example.com/e2e-testing',
                'provider': 'testing'
            }
        }]
        
        # Execute workflow
        result = await orchestrator.execute_complete_workflow(
            task_id=task_id,
            sources=sources,
            research_context="End-to-end testing methodologies"
        )
        
        # Verify results
        assert isinstance(result, DOKWorkflowResult)
        assert result.task_id == task_id
        assert len(result.source_summaries) == 1
        assert len(result.knowledge_tree) >= 1
        assert len(result.insights) >= 1
        assert len(result.spiky_povs["truth"]) >= 1
        assert len(result.spiky_povs["myth"]) >= 1
        assert result.workflow_stats["workflow_completion"] is True
        
        print(f"✅ E2E workflow test completed successfully for task {task_id}")
        
        await kb.disconnect()
        
    except Exception as e:
        pytest.skip(f"E2E workflow test skipped due to setup: {e}")


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
